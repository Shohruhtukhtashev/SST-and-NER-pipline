{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10873235,"sourceType":"datasetVersion","datasetId":6755477}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install necessary library","metadata":{}},{"cell_type":"code","source":"! pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T10:15:25.684311Z","iopub.execute_input":"2025-03-10T10:15:25.684744Z","iopub.status.idle":"2025-03-10T10:15:32.505986Z","shell.execute_reply.started":"2025-03-10T10:15:25.684705Z","shell.execute_reply":"2025-03-10T10:15:32.504919Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=3511e5a59adb7a6757666f9e1c2bd2e5e0db0bd636bfbb2a16dc71c8fbebf5bf\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"risqaliyevds/uzbek_ner\", split=\"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Find best learning rate","metadata":{}},{"cell_type":"code","source":"# import necessary packages\nimport torch\nfrom transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, load_dataset\nimport numpy as np\nfrom seqeval.metrics import classification_report\n\n# Step 1: Load dataset from Hugging Face\ndef load_hf_dataset():\n    dataset = load_dataset(\"risqaliyevds/uzbek_ner\", split=\"train\")\n    return dataset\n\n# Step 2: Preprocess the dataset for NER (convert to BIO format with specific labels)\ndef preprocess_dataset(dataset):\n    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n    allowed_entities = {'PERSON', 'DATE', 'LOC', 'ORG', 'LAW'}\n    label_set = set(['O'])\n\n    def process_example(example):\n        text = example['text']\n        ner = example['ner']\n\n        tokens = tokenizer(\n            text,\n            truncation=True,\n            max_length=512,\n            return_offsets_mapping=True\n        )\n        token_labels = ['O'] * len(tokens['input_ids'])\n\n        if ner is None:\n            pass\n        else:\n            for entity_type, entities in ner.items():\n                if entity_type not in allowed_entities:\n                    continue\n                    \n                label_set.add(f'B-{entity_type}')\n                label_set.add(f'I-{entity_type}')\n\n                if entities is None or not isinstance(entities, (list, tuple)):\n                    continue\n\n                for entity in entities:\n                    if not isinstance(entity, str):\n                        continue\n                    start = text.find(entity)\n                    if start == -1:\n                        continue\n                    end = start + len(entity)\n\n                    for i, (offset_start, offset_end) in enumerate(tokens['offset_mapping']):\n                        if offset_start >= start and offset_end <= end:\n                            if offset_start == start:\n                                token_labels[i] = f'B-{entity_type}'\n                            else:\n                                token_labels[i] = f'I-{entity_type}'\n\n        return {\n            'input_ids': tokens['input_ids'],\n            'attention_mask': tokens['attention_mask'],\n            'labels': token_labels\n        }\n\n    processed_dataset = dataset.map(process_example, remove_columns=['text', 'ner'])\n    label_list = sorted(list(label_set))\n    label2id = {label: idx for idx, label in enumerate(label_list)}\n\n    def convert_labels(example):\n        try:\n            labels = [label2id[label] for label in example['labels']]\n            padded_labels = labels + [-100] * (512 - len(labels))\n            example['labels'] = padded_labels\n        except KeyError as e:\n            raise\n        return example\n\n    processed_dataset = processed_dataset.map(convert_labels)\n    return processed_dataset, label_list, label2id, tokenizer\n\n# Step 3: Fine-tune XLM-RoBERTa\ndef fine_tune_model(dataset, label_list, label2id, tokenizer):\n    model = XLMRobertaForTokenClassification.from_pretrained(\n        'xlm-roberta-base',\n        num_labels=len(label_list),\n        id2label={i: label for i, label in enumerate(label_list)},\n        label2id=label2id\n    )\n\n    learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]\n    best_trainer = None\n    best_model = None\n    best_lr = None\n    best_eval_results = None\n\n    def data_collator(features):\n        batch = tokenizer.pad(\n            features,\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        max_len = batch['input_ids'].shape[1]\n        batch['labels'] = torch.tensor(\n            [f['labels'][:max_len] + [-100] * (max_len - len(f['labels'][:max_len])) for f in features],\n            dtype=torch.long\n        )\n        return batch\n\n    train_test_split = dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split['train']\n    eval_dataset = train_test_split['test']\n\n    for lr in learning_rates:\n        print(f\"Testing learning rate: {lr}\")\n        training_args = TrainingArguments(\n            output_dir=f'./results_lr_{lr}',\n            num_train_epochs=1,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            per_device_eval_batch_size=4,\n            warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir=f'./logs_lr_{lr}',\n            logging_steps=100,\n            fp16=True,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            report_to=\"none\",\n            learning_rate=lr,\n            lr_scheduler_type=\"linear\"\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            data_collator=data_collator,\n            compute_metrics=lambda p: compute_metrics(p, label_list)\n        )\n\n        trainer.train()\n        eval_results = trainer.evaluate()\n        print(f\"Learning rate {lr} - Evaluation results: {eval_results}\")\n\n        # Track the best performing model based on F1 score\n        if best_eval_results is None or eval_results['eval_f1'] > best_eval_results['eval_f1']:\n            best_trainer = trainer\n            best_model = model\n            best_lr = lr\n            best_eval_results = eval_results\n\n    return best_trainer, best_model, best_lr, best_eval_results\n\n# Step 4: Compute metrics\ndef compute_metrics(pred, label_list):\n    predictions, labels = pred\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n    pred_labels = [[label_list[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n\n    results = classification_report(true_labels, pred_labels, output_dict=True)\n    return {\n        \"precision\": results[\"micro avg\"][\"precision\"],\n        \"recall\": results[\"micro avg\"][\"recall\"],\n        \"f1\": results[\"micro avg\"][\"f1-score\"],\n    }\n\n# Main execution\nif __name__ == \"__main__\":\n    dataset = load_hf_dataset()\n    processed_dataset, label_list, label2id, tokenizer = preprocess_dataset(dataset)\n    trainer, model, best_lr, eval_results = fine_tune_model(processed_dataset, label_list, label2id, tokenizer)\n    print(f\"Best learning rate {best_lr} - Final evaluation results: {eval_results}\")\n    model.save_pretrained(\"./ner_model\")\n    tokenizer.save_pretrained(\"./ner_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T08:39:47.021659Z","iopub.execute_input":"2025-03-10T08:39:47.021987Z","iopub.status.idle":"2025-03-10T09:33:52.210460Z","shell.execute_reply.started":"2025-03-10T08:39:47.021962Z","shell.execute_reply":"2025-03-10T09:33:52.209472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e95e038ff749e8b73f8fb8ec633486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uzbek_ner.json:   0%|          | 0.00/24.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d35646df16a34195b041d413326e163f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15570ae2d3c8496cb80ec92fd736058c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43b2e8794e543eb864ccf66638e4487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32377e64840841c396d0d11c7ed1ef21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc9a130f32e4b638be8c1f4cab0ac8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0b8f54668d45a682f15bde9e12208f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c08a33a119d420ba5e3b83a8e7403b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0751fde0fbe1411cbf65df0f7be9006f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1bd019aae04006bd30ab3e5d5a4847"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Testing learning rate: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.144500</td>\n      <td>0.138469</td>\n      <td>0.500501</td>\n      <td>0.482579</td>\n      <td>0.491376</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Learning rate 1e-05 - Evaluation results: {'eval_loss': 0.1384693831205368, 'eval_precision': 0.5005005005005005, 'eval_recall': 0.48257890165041983, 'eval_f1': 0.49137634514274486, 'eval_runtime': 23.8043, 'eval_samples_per_second': 82.38, 'eval_steps_per_second': 20.627, 'epoch': 1.0}\nTesting learning rate: 2e-05\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.131000</td>\n      <td>0.126477</td>\n      <td>0.549881</td>\n      <td>0.624554</td>\n      <td>0.584843</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Learning rate 2e-05 - Evaluation results: {'eval_loss': 0.12647707760334015, 'eval_precision': 0.5498810333106731, 'eval_recall': 0.6245536145159734, 'eval_f1': 0.5848434181390936, 'eval_runtime': 23.6741, 'eval_samples_per_second': 82.833, 'eval_steps_per_second': 20.74, 'epoch': 1.0}\nTesting learning rate: 3e-05\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.130000</td>\n      <td>0.126789</td>\n      <td>0.565236</td>\n      <td>0.631792</td>\n      <td>0.596664</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Learning rate 3e-05 - Evaluation results: {'eval_loss': 0.1267893761396408, 'eval_precision': 0.5652361626802521, 'eval_recall': 0.6317922980407297, 'eval_f1': 0.5966639321848509, 'eval_runtime': 23.5914, 'eval_samples_per_second': 83.123, 'eval_steps_per_second': 20.813, 'epoch': 1.0}\nTesting learning rate: 5e-05\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.128600</td>\n      <td>0.127545</td>\n      <td>0.577286</td>\n      <td>0.626484</td>\n      <td>0.600879</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Learning rate 5e-05 - Evaluation results: {'eval_loss': 0.1275448054075241, 'eval_precision': 0.5772856634649591, 'eval_recall': 0.6264839301225751, 'eval_f1': 0.6008794260587828, 'eval_runtime': 23.6476, 'eval_samples_per_second': 82.926, 'eval_steps_per_second': 20.763, 'epoch': 1.0}\nBest learning rate 5e-05 - Final evaluation results: {'eval_loss': 0.1275448054075241, 'eval_precision': 0.5772856634649591, 'eval_recall': 0.6264839301225751, 'eval_f1': 0.6008794260587828, 'eval_runtime': 23.6476, 'eval_samples_per_second': 82.926, 'eval_steps_per_second': 20.763, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Best learning rate 5e-05","metadata":{}},{"cell_type":"markdown","source":"### **default learning rate 5e-5**","metadata":{}},{"cell_type":"markdown","source":"# Find best **scheduler** and **weight decays**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import TrainingArguments, Trainer, XLMRobertaTokenizerFast, XLMRobertaForTokenClassification\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\nfrom seqeval.metrics import classification_report\n\n# Parametrlar ro'yxati\nlr_scheduler_types = [\"linear\", \"cosine\", \"constant\"]\nweight_decays = [0.01, 0.1, 0.001]\n\n# Datasetni yuklash\ndef load_hf_dataset():\n    dataset = load_dataset(\"risqaliyevds/uzbek_ner\", split=\"train\")\n    return dataset\n\n# Datasetni tayyorlash\ndef preprocess_dataset(dataset):\n    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n    allowed_entities = {'PERSON', 'DATE', 'LOC', 'ORG', 'LAW'}\n    label_set = set(['O'])\n\n    def process_example(example):\n        text = example['text']\n        ner = example['ner']\n        tokens = tokenizer(text, truncation=True, max_length=512, return_offsets_mapping=True)\n        token_labels = ['O'] * len(tokens['input_ids'])\n        \n        if ner:\n            for entity_type, entities in ner.items():\n                if entities and entity_type in allowed_entities:\n                    label_set.add(f'B-{entity_type}')\n                    label_set.add(f'I-{entity_type}')\n                    for entity in entities:\n                        start = text.find(entity)\n                        end = start + len(entity)\n                        for i, (offset_start, offset_end) in enumerate(tokens['offset_mapping']):\n                            if offset_start >= start and offset_end <= end:\n                                token_labels[i] = f'B-{entity_type}' if offset_start == start else f'I-{entity_type}'\n        \n        return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask'], 'labels': token_labels}\n    \n    processed_dataset = dataset.map(process_example, remove_columns=['text', 'ner'])\n    label_list = sorted(list(label_set))\n    label2id = {label: idx for idx, label in enumerate(label_list)}\n    \n    def convert_labels(example):\n        example['labels'] = [label2id[label] for label in example['labels']] + [-100] * (512 - len(example['labels']))\n        return example\n    \n    processed_dataset = processed_dataset.map(convert_labels)\n    return processed_dataset, label_list, label2id, tokenizer\n\n# Modelni fine-tune qilish\ndef fine_tune_model(dataset, label_list, label2id, tokenizer, scheduler, wd):\n    model = XLMRobertaForTokenClassification.from_pretrained(\n        'xlm-roberta-base',\n        num_labels=len(label_list),\n        id2label={i: label for i, label in enumerate(label_list)},\n        label2id=label2id\n    )\n    \n    def data_collator(features):\n        batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n        max_len = batch['input_ids'].shape[1]\n        batch['labels'] = torch.tensor(\n            [f['labels'][:max_len] + [-100] * (max_len - len(f['labels'][:max_len])) for f in features], dtype=torch.long\n        )\n        return batch\n    \n    train_test_split = dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split['train']\n    eval_dataset = train_test_split['test']\n    \n    training_args = TrainingArguments(\n        output_dir=\"./results_temp\",  # Natijalarni saqlamaslik\n        num_train_epochs=1,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        per_device_eval_batch_size=4,\n        warmup_steps=500,\n        weight_decay=wd,\n        logging_dir=None,  # Loglarni o‘chirish\n        logging_steps=500,\n        fp16=True,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  # Modelni saqlamaslik\n        load_best_model_at_end=False,\n        report_to=\"none\",\n        lr_scheduler_type=scheduler\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        compute_metrics=lambda p: compute_metrics(p, label_list)\n    )\n    \n    trainer.train()\n    eval_results = trainer.evaluate()\n    return eval_results[\"eval_f1\"]\n\n# Baholash metrikasi\ndef compute_metrics(pred, label_list):\n    predictions, labels = pred\n    predictions = np.argmax(predictions, axis=2)\n    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n    pred_labels = [[label_list[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n    results = classification_report(true_labels, pred_labels, output_dict=True)\n    return {\"f1\": results[\"micro avg\"][\"f1-score\"]}\n\n# Hyperparameter tuning\nif __name__ == \"__main__\":\n    dataset = load_hf_dataset()\n    processed_dataset, label_list, label2id, tokenizer = preprocess_dataset(dataset)\n    \n    best_f1 = 0\n    best_params = None\n    \n    for idx, (scheduler, wd) in enumerate(itertools.product(lr_scheduler_types, weight_decays)):\n        print(f\"Testing combination {idx+1}/{len(lr_scheduler_types) * len(weight_decays)}: Scheduler={scheduler}, WD={wd}\")\n        f1_score = fine_tune_model(processed_dataset, label_list, label2id, tokenizer, scheduler, wd)\n        print(f\"Combination {idx+1} - F1 Score: {f1_score}\")\n        \n        if f1_score > best_f1:\n            best_f1 = f1_score\n            best_params = (scheduler, wd)\n    \n    print(f\"\\nBest combination: Scheduler={best_params[0]}, Weight Decay={best_params[1]}\")\n    print(f\"Best F1 Score: {best_f1}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T10:18:28.123580Z","iopub.execute_input":"2025-03-10T10:18:28.123948Z","iopub.status.idle":"2025-03-10T12:16:28.855956Z","shell.execute_reply.started":"2025-03-10T10:18:28.123920Z","shell.execute_reply":"2025-03-10T12:16:28.855179Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb970bf6012a4610b2acc8cf553866c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad4679c915242d0996021ab205880e1"}},"metadata":{}},{"name":"stdout","text":"Testing combination 1/9: Scheduler=linear, WD=0.01\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622e57abe2634a1dbc3440fe7c2ae0cf"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.160100</td>\n      <td>0.146766</td>\n      <td>0.549754</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 1 - F1 Score: 0.5497542739530391\nTesting combination 2/9: Scheduler=linear, WD=0.1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.158900</td>\n      <td>0.152470</td>\n      <td>0.545524</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 2 - F1 Score: 0.5455235524817611\nTesting combination 3/9: Scheduler=linear, WD=0.001\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.157600</td>\n      <td>0.152044</td>\n      <td>0.546296</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 3 - F1 Score: 0.5462962962962964\nTesting combination 4/9: Scheduler=cosine, WD=0.01\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.156100</td>\n      <td>0.153646</td>\n      <td>0.544230</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 4 - F1 Score: 0.5442298674169757\nTesting combination 5/9: Scheduler=cosine, WD=0.1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:38, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.155900</td>\n      <td>0.153202</td>\n      <td>0.537439</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 5 - F1 Score: 0.5374389466978127\nTesting combination 6/9: Scheduler=cosine, WD=0.001\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.156100</td>\n      <td>0.153378</td>\n      <td>0.543780</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 6 - F1 Score: 0.5437799753830483\nTesting combination 7/9: Scheduler=constant, WD=0.01\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.165500</td>\n      <td>0.161418</td>\n      <td>0.533405</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 7 - F1 Score: 0.5334048168921147\nTesting combination 8/9: Scheduler=constant, WD=0.1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.165300</td>\n      <td>0.161431</td>\n      <td>0.532701</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 8 - F1 Score: 0.5327014218009479\nTesting combination 9/9: Scheduler=constant, WD=0.001\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2206/2206 12:36, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.165700</td>\n      <td>0.161938</td>\n      <td>0.526644</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Combination 9 - F1 Score: 0.5266436848444573\n\nBest combination: Scheduler=linear, Weight Decay=0.01\nBest F1 Score: 0.5497542739530391\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Best combination: Scheduler=linear, Weight Decay=0.01\n### Best F1 Score: 0.5497542739530391","metadata":{}}]}