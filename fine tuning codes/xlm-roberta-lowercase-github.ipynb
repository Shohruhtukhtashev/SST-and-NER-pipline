{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install seqeval\n!pip install unidecode","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:11.683578Z","iopub.execute_input":"2025-03-11T16:13:11.683861Z","iopub.status.idle":"2025-03-11T16:13:22.210237Z","shell.execute_reply.started":"2025-03-11T16:13:11.683831Z","shell.execute_reply":"2025-03-11T16:13:22.209231Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=de20e46ee3b9c55dc375b7985225b53aaaf4a15e5069856ba48b01fa7f4717f9\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nCollecting unidecode\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nfrom tqdm import tqdm\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:22.212367Z","iopub.execute_input":"2025-03-11T16:13:22.212682Z","iopub.status.idle":"2025-03-11T16:13:24.004894Z","shell.execute_reply.started":"2025-03-11T16:13:22.212654Z","shell.execute_reply":"2025-03-11T16:13:24.003996Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_json(\"hf://datasets/risqaliyevds/uzbek_ner/uzbek_ner.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:24.010991Z","iopub.execute_input":"2025-03-11T16:13:24.011336Z","iopub.status.idle":"2025-03-11T16:13:25.044438Z","shell.execute_reply.started":"2025-03-11T16:13:24.011302Z","shell.execute_reply":"2025-03-11T16:13:25.043531Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Convert Roman numerals and numbers replace to words.","metadata":{}},{"cell_type":"code","source":"# 1. Roman digits convert to words.\ndef roman2digit(s):\n    roman = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000,'IV':4,'IX':9,'XL':40,'XC':90,'CD':400,'CM':900}\n    i = 0\n    num = 0\n    while i < len(s):\n        if i+1<len(s) and s[i:i+2] in roman:\n            num+=roman[s[i:i+2]]\n            i+=2\n        else:\n            num+=roman[s[i]]\n            i+=1\n    return num\n\ndef replace_roman(match):\n    matched = match.group(2)\n    # IIV is not roman digit. It means \"Ichki ishlar vazirligi\"\n    if matched == 'IIV':\n        return match.group(1)+'iiv'+match.group(3)\n    number = roman2digit(matched)\n    word = num2word(number)\n    if word[-1] == 'i' or word[-1] == 'a':\n        word += 'nchi'\n    else:\n        word += 'inchi'\n    if match.group(1)=='-' or match.group(3)=='-':\n        return ' '+word+' '\n    else:\n        return match.group(1)+word+match.group(3)\n\n\n# 2. Numbers to words\ndef three_digit(a):\n    yuz = a // 100\n    on = a // 10 % 10\n    bir = a % 10\n    word = ''\n    # yuzlar xonasi\n\n    if yuz == 1:\n        word +=\"bir\"\n    elif yuz == 2:\n        word += \"ikki\"\n    elif yuz == 3:\n        word += \"uch\"\n    elif yuz == 4:\n        word += \"to ªrt\"\n    elif yuz == 5:\n        word += \"besh\"\n    elif yuz == 6:\n        word += \"olti\"\n    elif yuz == 7:\n        word += \"yetti\"\n    elif yuz == 8:\n        word += \"sakkiz\"\n    elif yuz == 9:\n        word += \"to ªqqiz\"\n    if yuz != 0:\n        word += \" yuz\"\n\n    # o'nlar xonasi\n\n    if on == 1:\n        word += \" o ªn\"\n    elif on == 2:\n        word += \" yigirma\"\n    elif on == 3:\n        word += \" o ªttiz\"\n    elif on == 4:\n        word += \" qirq\"\n    elif on == 5:\n        word += \" ellik\"\n    elif on == 6:\n        word += \" oltmish\"\n    elif on == 7:\n        word += \" yetmish\"\n    elif on == 8:\n        word += \" sakson\"\n    elif on == 9:\n        word += \" to ªqson\"\n\n    # birlar xonasi\n\n    if bir == 1:\n        word +=\" bir\"\n    elif bir == 2:\n        word += \" ikki\"\n    elif bir == 3:\n        word += \" uch\"\n    elif bir == 4:\n        word += \" to ªrt\"\n    elif bir == 5:\n        word += \" besh\"\n    elif bir == 6:\n        word += \" olti\"\n    elif bir == 7:\n        word += \" yetti\"\n    elif bir == 8:\n        word += \" sakkiz\"\n    elif bir == 9:\n        word += \" to ªqqiz\"\n    \n    return word\n\ndef num2word(n):\n    if n == 0:\n        return 'nol'\n    names = [\"\", \"ming\", \"million\", \"milliard\", \"trillion\", \"kvadrillion\", \"kvintillion\", \"sekstillion\", \"septillion\", \"oktalon\", \"nonalon\", \"dekalon\", \"endekalon\", \"dodekalon\"]\n    digit = 0\n    word = ''\n    d = n\n    while d > 0:\n        d //= 10\n        digit += 1\n\n    if digit % 3 == 0:\n        x = 0\n    else:\n        x = 1\n    while n > 0:\n        if x:\n            k = n // 10 ** (digit - digit % 3)\n            n %= 10 ** (digit - digit % 3)\n        else:\n            k = n // 10 ** (digit-3)\n            n %= 10 ** (digit-3)\n        word += three_digit(k)+' '\n        if x:\n            word += names[digit//3]+' '\n        else:\n            word += names[digit//3-1]+' '\n        if x:\n            digit -= digit % 3\n            x = 0\n        else:\n            digit -= 3\n\n    return word.strip()\n\ndef float_num2word(n):\n    tens = [' o ªndan ', 'yuzdan ', 'mingdan ', ' o ªn mingdan ', ' yuz mingdan ', ' milliondan ']\n    whole = n.split('.')[0]\n    frac = n.split('.')[1]\n    if frac == '0':\n        return num2word(int(whole))\n    return num2word(int(whole)) + ' butun ' + tens[len(frac) - 1]+num2word(int(frac))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:26.954573Z","iopub.execute_input":"2025-03-11T16:13:26.954866Z","iopub.status.idle":"2025-03-11T16:13:26.971663Z","shell.execute_reply.started":"2025-03-11T16:13:26.954837Z","shell.execute_reply":"2025-03-11T16:13:26.970929Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### List of characters that should remain in the dataset, convert to lowercase","metadata":{}},{"cell_type":"code","source":"\ndef replace_digit(match):\n    if len(match.groups()) == 3:\n        # I. ':' orqali ajratilgan raqamlar -> \"u\" yoki \"yu\" qo‚Äòshish sharti bilan.\n        first = num2word(int(match.group(1)))\n        second = num2word(int(match.group(3)))\n\n        if match.group(3)[-1] in \"134579\":  # Agar oxirgi raqam 1, 3, 4, 5, 7, 9 bo'lsa\n            return first + \"u \" + second\n        else:\n            return first + \"yu \" + second\n\n    elif len(match.groups()) == 2:\n        # II. Son va '-' bo‚Äòlsa uni tartib raqam shakliga o‚Äòtkazish\n        num = num2word(int(match.group(1)))\n        if num[-1] in \"ai\":  # Agar oxirgi harf 'a' yoki 'i' bo‚Äòlsa\n            return num + \"nchi \"\n        else:\n            return num + \"inchi \"\n    \n    else:\n        # III. Barcha boshqa sonlarni oddiy so‚Äòz shakliga o‚Äòtkazish\n        return num2word(int(match.group()))\n\n\n# Tozalash funksiyasi\ndef clean_text(df):\n    characters = [\n        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n\n        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', \n        'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', \n        'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n\n        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', \n        'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n        's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n\n        ' ', \" ª\" ,\"‚Ä≤\", \" º\", \"‚Äô\", \"'\", \"‚Äò\", '\\n', '\\-',':','>'\n    ]\n\n    for i in tqdm(df.index):\n        # Raqamli yozuvlarni almashtirish\n        if re.search(r\"\\d+\\:\\d+\", df.loc[i, 'text']):\n            df.loc[i, 'text'] = re.sub(r\"(\\d+)(\\:)(\\d+)\", replace_digit, df.loc[i, 'text'])\n        if re.search(r\"\\d(\\.|,)\\d\", df.loc[i, 'text']):\n            df.loc[i, 'text'] = re.sub(r\"(\\d+)(\\.|,)(\\d+)\", replace_digit, df.loc[i, 'text'])\n        if re.search(r\"\\d+-\", df.loc[i, 'text']):\n            df.loc[i, 'text'] = re.sub(r\"(\\d+)(-)\", replace_digit, df.loc[i, 'text'])\n        if re.search(r\"\\d+\", df.loc[i, 'text']):\n            df.loc[i, 'text'] = re.sub(r\"\\d+\", replace_digit, df.loc[i, 'text'])\n\n        # Belgilarni tozalash\n        df.loc[i, 'text'] = re.sub(f\"[^{''.join(characters)}]\", r\" \", df.loc[i, 'text'])\n\n        df.loc[i, 'text'] = re.sub(r'-',' ', df.loc[i, 'text'])\n        df.loc[i, 'text'] = df.loc[i, 'text'].lower().strip()\n\n    return df  # Tozalangan DataFrame`ni qaytarish\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:26.972410Z","iopub.execute_input":"2025-03-11T16:13:26.972670Z","iopub.status.idle":"2025-03-11T16:13:26.985458Z","shell.execute_reply.started":"2025-03-11T16:13:26.972650Z","shell.execute_reply":"2025-03-11T16:13:26.984844Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Remove unnecessary characters","metadata":{}},{"cell_type":"code","source":"clean_text(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:26.986333Z","iopub.execute_input":"2025-03-11T16:13:26.986612Z","iopub.status.idle":"2025-03-11T16:13:37.095334Z","shell.execute_reply.started":"2025-03-11T16:13:26.986585Z","shell.execute_reply":"2025-03-11T16:13:37.094492Z"}},"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19609/19609 [00:10<00:00, 1948.09it/s]\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                    text  \\\n0      shvetsiya hukumati stokholmdagi asosiy piyodal...   \n1      turkiya prezidenti rajab toyyib erdo‚Äòg‚Äòan aqsh...   \n2      stokholm markazida yuk mashinasi orqali sodir ...   \n3      vest hem  bosh murabbiyi slaven bilich o‚Äòz vaz...   \n4      aqsh prezidenti donald trampning nabirasi   be...   \n...                                                  ...   \n19604  ikki ming  yigirma ikkinchi ikki ming  yigirma...   \n19605  o‚Äòzbekistonda erkaklar o‚Äòrtacha yigirma oltiu ...   \n19606  konstitutsion islohotlar muhokamasiga oid yig‚Äò...   \n19607  toshkent shahrida issiq suv ta‚Äôminoti vaqtinch...   \n19608  o‚Äòzbekiston prezidenti shavkat mirziyoyev hind...   \n\n                                                     ner  \n0      {'GPE': ['Shvetsiya', 'O‚Äòzbekiston', 'Shvetsiy...  \n1      {'GPE': ['O‚Äòzbekiston', 'Suriya', 'AQSh', 'Vas...  \n2      {'LOC': ['Stokholm', 'Stokgolm'], 'GPE': ['O‚Äòz...  \n3      {'GPE': ['O‚Äòzbekiston', 'Angliya'], 'ORG': ['V...  \n4      {'PERSON': ['Donald Tramp', 'Ivanka Tramp', 'S...  \n...                                                  ...  \n19604  {'GPE': ['O‚Äòzbekiston', 'Qoraqalpog‚Äòiston Resp...  \n19605  {'GPE': ['O‚Äòzbekiston', 'Qoraqalpog‚Äòiston', 'A...  \n19606  {'LOC': ['Toshkent', 'O‚Äòzbekiston'], 'ORG': ['...  \n19607  {'LOC': ['Toshkent', 'Mirobod', 'Yakkasaroy', ...  \n19608  {'GPE': ['O‚Äòzbekiston', 'Hindiston', 'Assam', ...  \n\n[19609 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>ner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>shvetsiya hukumati stokholmdagi asosiy piyodal...</td>\n      <td>{'GPE': ['Shvetsiya', 'O‚Äòzbekiston', 'Shvetsiy...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>turkiya prezidenti rajab toyyib erdo‚Äòg‚Äòan aqsh...</td>\n      <td>{'GPE': ['O‚Äòzbekiston', 'Suriya', 'AQSh', 'Vas...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stokholm markazida yuk mashinasi orqali sodir ...</td>\n      <td>{'LOC': ['Stokholm', 'Stokgolm'], 'GPE': ['O‚Äòz...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>vest hem  bosh murabbiyi slaven bilich o‚Äòz vaz...</td>\n      <td>{'GPE': ['O‚Äòzbekiston', 'Angliya'], 'ORG': ['V...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aqsh prezidenti donald trampning nabirasi   be...</td>\n      <td>{'PERSON': ['Donald Tramp', 'Ivanka Tramp', 'S...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19604</th>\n      <td>ikki ming  yigirma ikkinchi ikki ming  yigirma...</td>\n      <td>{'GPE': ['O‚Äòzbekiston', 'Qoraqalpog‚Äòiston Resp...</td>\n    </tr>\n    <tr>\n      <th>19605</th>\n      <td>o‚Äòzbekistonda erkaklar o‚Äòrtacha yigirma oltiu ...</td>\n      <td>{'GPE': ['O‚Äòzbekiston', 'Qoraqalpog‚Äòiston', 'A...</td>\n    </tr>\n    <tr>\n      <th>19606</th>\n      <td>konstitutsion islohotlar muhokamasiga oid yig‚Äò...</td>\n      <td>{'LOC': ['Toshkent', 'O‚Äòzbekiston'], 'ORG': ['...</td>\n    </tr>\n    <tr>\n      <th>19607</th>\n      <td>toshkent shahrida issiq suv ta‚Äôminoti vaqtinch...</td>\n      <td>{'LOC': ['Toshkent', 'Mirobod', 'Yakkasaroy', ...</td>\n    </tr>\n    <tr>\n      <th>19608</th>\n      <td>o‚Äòzbekiston prezidenti shavkat mirziyoyev hind...</td>\n      <td>{'GPE': ['O‚Äòzbekiston', 'Hindiston', 'Assam', ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>19609 rows √ó 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"**Create empty dataframe**","metadata":{}},{"cell_type":"code","source":"df_ner = pd.DataFrame()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:38.678981Z","iopub.execute_input":"2025-03-11T16:13:38.679255Z","iopub.status.idle":"2025-03-11T16:13:38.692066Z","shell.execute_reply.started":"2025-03-11T16:13:38.679236Z","shell.execute_reply":"2025-03-11T16:13:38.691287Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Clean the values of the NER column","metadata":{}},{"cell_type":"code","source":"df_list=[]\nfor i in df['ner']:\n    list_set = list(i.values())\n    for j in list_set:\n        df_list.append('>'.join(j))\n\ndf_ner['text'] = df_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:38.692791Z","iopub.execute_input":"2025-03-11T16:13:38.692997Z","iopub.status.idle":"2025-03-11T16:13:38.756891Z","shell.execute_reply.started":"2025-03-11T16:13:38.692979Z","shell.execute_reply":"2025-03-11T16:13:38.756253Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Remove unnecessary characters","metadata":{}},{"cell_type":"code","source":"clean_text(df_ner)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:13:39.636114Z","iopub.execute_input":"2025-03-11T16:13:39.636418Z","iopub.status.idle":"2025-03-11T16:14:06.068667Z","shell.execute_reply.started":"2025-03-11T16:13:39.636392Z","shell.execute_reply":"2025-03-11T16:14:06.067936Z"}},"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86569/86569 [00:26<00:00, 3277.68it/s]\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                                    text\n0      shvetsiya>o‚Äòzbekiston>shvetsiya bosh vaziri st...\n1                          drottninggatanda>stokholmdagi\n2                                  spendrups kompaniyasi\n3                    shvetsiya bosh vaziri stefan lyoven\n4                     o‚Äòzbekiston>suriya>aqsh>vashington\n...                                                  ...\n86564                                            toshiem\n86565              o‚Äòzbekiston>hindiston>assam>megxalaya\n86566  hindiston shimoli sharqi>shimoli sharqiy assam...\n86567  o‚Äòzbekiston prezidenti>hindiston prezidenti>hi...\n86568  shavkat mirziyoyev>ram nath kovind>narendra mo...\n\n[86569 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>shvetsiya&gt;o‚Äòzbekiston&gt;shvetsiya bosh vaziri st...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>drottninggatanda&gt;stokholmdagi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spendrups kompaniyasi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>shvetsiya bosh vaziri stefan lyoven</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>o‚Äòzbekiston&gt;suriya&gt;aqsh&gt;vashington</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>86564</th>\n      <td>toshiem</td>\n    </tr>\n    <tr>\n      <th>86565</th>\n      <td>o‚Äòzbekiston&gt;hindiston&gt;assam&gt;megxalaya</td>\n    </tr>\n    <tr>\n      <th>86566</th>\n      <td>hindiston shimoli sharqi&gt;shimoli sharqiy assam...</td>\n    </tr>\n    <tr>\n      <th>86567</th>\n      <td>o‚Äòzbekiston prezidenti&gt;hindiston prezidenti&gt;hi...</td>\n    </tr>\n    <tr>\n      <th>86568</th>\n      <td>shavkat mirziyoyev&gt;ram nath kovind&gt;narendra mo...</td>\n    </tr>\n  </tbody>\n</table>\n<p>86569 rows √ó 1 columns</p>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"### Restore the NER column to its original structure","metadata":{}},{"cell_type":"code","source":"index=0\n\nfor i in tqdm(df.index):\n    for j in df['ner'][i].keys():\n        ner_item = df_ner.text[index].split('>')\n        df['ner'][i][j] = ner_item\n        index += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:14:06.936920Z","iopub.execute_input":"2025-03-11T16:14:06.937202Z","iopub.status.idle":"2025-03-11T16:14:08.109856Z","shell.execute_reply.started":"2025-03-11T16:14:06.937181Z","shell.execute_reply":"2025-03-11T16:14:08.109178Z"}},"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19609/19609 [00:01<00:00, 16965.88it/s]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"#### DataFrame to hugging face dataset format","metadata":{}},{"cell_type":"code","source":"dataset = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:14:08.129983Z","iopub.execute_input":"2025-03-11T16:14:08.130293Z","iopub.status.idle":"2025-03-11T16:14:08.542044Z","shell.execute_reply.started":"2025-03-11T16:14:08.130266Z","shell.execute_reply":"2025-03-11T16:14:08.541149Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### Fine tuning","metadata":{}},{"cell_type":"markdown","source":"The model will be fine-tuned to recognize PERSON, DATE, LOC, ORG, and GPE entities. Use best hyperparametrs","metadata":{}},{"cell_type":"code","source":"# import necessary packages\nimport torch\nfrom transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, load_dataset\nimport numpy as np\nfrom seqeval.metrics import classification_report\n\n# Preprocess the dataset for NER (convert to BIO format with specific labels)\ndef preprocess_dataset(dataset):\n    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n    # Define the allowed entity types\n    allowed_entities = {'PERSON', 'DATE', 'LOC', 'ORG', 'GPE'}\n    label_set = set(['O'])  # Start with 'O' label\n\n    def process_example(example):\n        text = example['text']\n        ner = example['ner']\n\n        # Tokenize with padding and truncation enabled\n        tokens = tokenizer(\n            text,\n            truncation=True,\n            max_length=512,\n            return_offsets_mapping=True\n        )\n        token_labels = ['O'] * len(tokens['input_ids'])  # Default all to 'O'\n\n        if ner is None:\n            pass\n        else:\n            for entity_type, entities in ner.items():\n                # Only process allowed entity types\n                if entity_type not in allowed_entities:\n                    continue\n                    \n                label_set.add(f'B-{entity_type}')\n                label_set.add(f'I-{entity_type}')\n\n                if entities is None or not isinstance(entities, (list, tuple)):\n                    continue\n\n                for entity in entities:\n                    if not isinstance(entity, str):\n                        continue\n                    start = text.find(entity)\n                    if start == -1:\n                        continue\n                    end = start + len(entity)\n\n                    for i, (offset_start, offset_end) in enumerate(tokens['offset_mapping']):\n                        if offset_start >= start and offset_end <= end:\n                            if offset_start == start:\n                                token_labels[i] = f'B-{entity_type}'\n                            else:\n                                token_labels[i] = f'I-{entity_type}'\n\n        return {\n            'input_ids': tokens['input_ids'],\n            'attention_mask': tokens['attention_mask'],\n            'labels': token_labels\n        }\n\n    # Apply preprocessing\n    processed_dataset = dataset.map(process_example, remove_columns=['text', 'ner'])\n    label_list = sorted(list(label_set))\n    label2id = {label: idx for idx, label in enumerate(label_list)}\n\n    # Convert labels to IDs and pad with -100\n    def convert_labels(example):\n        try:\n            labels = [label2id[label] for label in example['labels']]\n            # Pad labels to match max_length (512) with -100\n            padded_labels = labels + [-100] * (512 - len(labels))\n            example['labels'] = padded_labels\n        except KeyError as e:\n            raise\n        return example\n\n    processed_dataset = processed_dataset.map(convert_labels)\n    return processed_dataset, label_list, label2id, tokenizer\n\n# Step 3: Fine-tune XLM-RoBERTa\ndef fine_tune_model(dataset, label_list, label2id, tokenizer):\n    model = XLMRobertaForTokenClassification.from_pretrained(\n        'xlm-roberta-base',\n        num_labels=len(label_list),\n        id2label={i: label for i, label in enumerate(label_list)},\n        label2id=label2id\n    )\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=3,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        per_device_eval_batch_size=4,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=500,\n        fp16=True,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        report_to=\"none\"\n    )\n\n    # Use the tokenizer's padding directly in the collator\n    def data_collator(features):\n        batch = tokenizer.pad(\n            features,\n            padding=True,  # Dynamic padding to longest in batch\n            return_tensors=\"pt\"\n        )\n        # Ensure labels are padded to match input_ids length\n        max_len = batch['input_ids'].shape[1]\n        batch['labels'] = torch.tensor(\n            [f['labels'][:max_len] + [-100] * (max_len - len(f['labels'][:max_len])) for f in features],\n            dtype=torch.long\n        )\n        return batch\n\n    train_test_split = dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split['train']\n    eval_dataset = train_test_split['test']\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        compute_metrics=lambda p: compute_metrics(p, label_list)\n    )\n\n    trainer.train()\n    return trainer, model\n\n# Step 4: Compute metrics\ndef compute_metrics(pred, label_list):\n    predictions, labels = pred\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n    pred_labels = [[label_list[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n\n    results = classification_report(true_labels, pred_labels, output_dict=True)\n    return {\n        \"precision\": results[\"micro avg\"][\"precision\"],\n        \"recall\": results[\"micro avg\"][\"recall\"],\n        \"f1\": results[\"micro avg\"][\"f1-score\"],\n    }\n\n# Main execution\nif __name__ == \"__main__\":\n    processed_dataset, label_list, label2id, tokenizer = preprocess_dataset(dataset)\n    trainer, model = fine_tune_model(processed_dataset, label_list, label2id, tokenizer)\n    eval_results = trainer.evaluate()\n    print(\"Evaluation results:\", eval_results)\n    model.save_pretrained(\"./ner_model\")\n    tokenizer.save_pretrained(\"./ner_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:14:27.380750Z","iopub.execute_input":"2025-03-11T16:14:27.381068Z","iopub.status.idle":"2025-03-11T16:54:13.840909Z","shell.execute_reply.started":"2025-03-11T16:14:27.381046Z","shell.execute_reply":"2025-03-11T16:54:13.840158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2ade51191944d2b5ab539533214766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59fb54c0b03e455eb2e862391decf2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16501d05ab3549408961c05635ad5dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c7ec39e0054bb1b9184c061d392f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fc4be971054b93951e2514d49c3dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bed60a468364aa883efcb721a423da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8385d6d746d43f2b2fae01ae97c92dd"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6618' max='6618' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6618/6618 38:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.194900</td>\n      <td>0.178818</td>\n      <td>0.531551</td>\n      <td>0.539191</td>\n      <td>0.535344</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.166900</td>\n      <td>0.167024</td>\n      <td>0.535358</td>\n      <td>0.599570</td>\n      <td>0.565647</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.136100</td>\n      <td>0.166734</td>\n      <td>0.571025</td>\n      <td>0.613795</td>\n      <td>0.591638</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='491' max='491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [491/491 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.16673429310321808, 'eval_precision': 0.5710248811082776, 'eval_recall': 0.6137946362424063, 'eval_f1': 0.5916378048345057, 'eval_runtime': 24.0253, 'eval_samples_per_second': 81.622, 'eval_steps_per_second': 20.437, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from transformers import pipeline\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n\ntext = \"'Samarqanddanmikan yoki Toshkentdanmikan anig'ini bilmadim' dedi Shohruh umirov bilan bordi\"\nner = nlp(text)\n\nfor entity in ner:\n    print(entity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:07:23.574665Z","iopub.execute_input":"2025-03-11T17:07:23.574955Z","iopub.status.idle":"2025-03-11T17:07:23.597584Z","shell.execute_reply.started":"2025-03-11T17:07:23.574933Z","shell.execute_reply":"2025-03-11T17:07:23.596752Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"{'entity': 'B-LOC', 'score': 0.74778676, 'index': 2, 'word': 'Sam', 'start': 1, 'end': 4}\n{'entity': 'I-LOC', 'score': 0.76890993, 'index': 3, 'word': 'ar', 'start': 4, 'end': 6}\n{'entity': 'I-LOC', 'score': 0.77585006, 'index': 4, 'word': 'qan', 'start': 6, 'end': 9}\n{'entity': 'I-LOC', 'score': 0.7489506, 'index': 5, 'word': 'd', 'start': 9, 'end': 10}\n{'entity': 'B-LOC', 'score': 0.76504, 'index': 10, 'word': '‚ñÅToshkent', 'start': 24, 'end': 32}\n{'entity': 'B-PERSON', 'score': 0.9199773, 'index': 23, 'word': '‚ñÅSho', 'start': 65, 'end': 68}\n{'entity': 'I-PERSON', 'score': 0.925115, 'index': 24, 'word': 'h', 'start': 68, 'end': 69}\n{'entity': 'I-PERSON', 'score': 0.926187, 'index': 25, 'word': 'ruh', 'start': 69, 'end': 72}\n{'entity': 'I-PERSON', 'score': 0.93724865, 'index': 26, 'word': '‚ñÅum', 'start': 73, 'end': 75}\n{'entity': 'I-PERSON', 'score': 0.9402853, 'index': 27, 'word': 'i', 'start': 75, 'end': 76}\n{'entity': 'I-PERSON', 'score': 0.9250853, 'index': 28, 'word': 'rov', 'start': 76, 'end': 79}\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"### login hugging face","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:13:56.665116Z","iopub.execute_input":"2025-03-11T17:13:56.665441Z","iopub.status.idle":"2025-03-11T17:13:56.754248Z","shell.execute_reply.started":"2025-03-11T17:13:56.665413Z","shell.execute_reply":"2025-03-11T17:13:56.753391Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"### Upload model to hugging face","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\nusername = \"tukhtashevshohruh\"  # Hugging Face username\nrepo_name = \"xlm-roberta-base-lowercase-high-accuracy\"  # Hugging Face-dagi yangi repo nomi\nfull_repo_name = f\"{username}/{repo_name}\"\n\n# Hugging Face-da yangi repo yaratish (agar yo'q bo'lsa)\napi = HfApi()\napi.create_repo(repo_id=full_repo_name, private=False)\n\n# Model va tokenizatorni Hugging Face-ga yuklash\napi.upload_folder(\n    folder_path=\"./ner_model\",  # Saqlangan model papkasi\n    repo_id=full_repo_name,\n    commit_message=\"Fine-tuned model uploaded 11.03.2025\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T17:15:14.152252Z","iopub.execute_input":"2025-03-11T17:15:14.152582Z","iopub.status.idle":"2025-03-11T17:15:45.257042Z","shell.execute_reply.started":"2025-03-11T17:15:14.152552Z","shell.execute_reply":"2025-03-11T17:15:45.256168Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff049c513cec48819d726ad93ccc8d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bd410c87614611ad6315bbd305a213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ff166f46c24c45a4bf6f9a496fa8a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6960185b6664cf7b6a53cb1b9e89283"}},"metadata":{}},{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/tukhtashevshohruh/xlm-roberta-base-lowercase-high-accuracy/commit/8d5eb45576a8fc35cff8484e1134d6050507b159', commit_message='Fine-tuned model uploaded 11.03.2025', commit_description='', oid='8d5eb45576a8fc35cff8484e1134d6050507b159', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tukhtashevshohruh/xlm-roberta-base-lowercase-high-accuracy', endpoint='https://huggingface.co', repo_type='model', repo_id='tukhtashevshohruh/xlm-roberta-base-lowercase-high-accuracy'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":63}]}